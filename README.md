# Frozen Lake Reinforcement Learning Project

This project implements various reinforcement learning algorithms to solve the Frozen Lake problem.

## Project Structure

```
group[X]/
â”‚
â”œâ”€â”€ ðŸ“„ main.py                      # Main execution script - orchestrates all 7 algorithms
â”œâ”€â”€ ðŸ“„ environment.py               # FrozenLake environment with p() and r() methods
â”œâ”€â”€ ðŸ“„ model_based.py               # Policy iteration & value iteration algorithms
â”œâ”€â”€ ðŸ“„ model_free.py                # Sarsa & Q-Learning (tabular TD control)
â”œâ”€â”€ ðŸ“„ linear.py                    # Linear function approximation (Linear Sarsa, Linear Q-Learning)
â”œâ”€â”€ ðŸ“„ deep_rl.py                   # Deep Q-Network (CNN + replay buffer + target network)
â”œâ”€â”€ ðŸ“„ questions.py                 # Experimental analysis for Q1-Q3 (iteration counts, learning curves, parameter tuning)
â”œâ”€â”€ ðŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ðŸ“„ README.md                    # This file
â”‚
â”œâ”€â”€ ðŸ“„ output.txt                   # [GENERATED] Required submission output from main.py
â”œâ”€â”€ ðŸ“„ p.npy                        # [PROVIDED] Validation file for environment correctness
â”‚
â”œâ”€â”€ ðŸ“ figures/                     # [GENERATED] Plots from questions.py
â”‚   â”œâ”€â”€ q2_sarsa_curve.png          # Sarsa learning curve (moving average)
â”‚   â”œâ”€â”€ q2_qlearning_curve.png      # Q-Learning learning curve
â”‚   â”œâ”€â”€ q2_linear_sarsa_curve.png   # Linear Sarsa learning curve
â”‚   â”œâ”€â”€ q2_linear_qlearning_curve.png # Linear Q-Learning learning curve
â”‚   â”œâ”€â”€ q2_dqn_curve.png            # DQN learning curve
â”‚   â””â”€â”€ q3_parameter_tuning.png     # Parameter tuning heatmap
â”‚
â””â”€â”€ ðŸ“ part2/                       # Part 2: Custom experiments
    â”œâ”€â”€ ðŸ“„ experiments.py           # Minigrid + Stable Baselines3 experiments
    â”œâ”€â”€ ðŸ“„ analysis.py              # Statistical analysis and visualization
    â”œâ”€â”€ ðŸ“„ README.md                # Part 2 detailed documentation
    â””â”€â”€ ðŸ“ results/                 # Experimental data and plots
        â”œâ”€â”€ training_logs.csv
        â””â”€â”€ plots/
```

### File Descriptions

**Core Implementation (Part 1):**
- **`environment.py`**: Implements FrozenLake grid world with transition probabilities `p()` and reward function `r()`. Handles slip mechanics, boundaries, and absorbing states.
- **`model_based.py`**: Contains policy evaluation, policy improvement, policy iteration, and value iteration algorithms. Uses Bellman equations with known environment dynamics.
- **`model_free.py`**: Implements Sarsa (on-policy) and Q-Learning (off-policy) algorithms with Îµ-greedy exploration and linear decay.
- **`linear.py`**: Linear function approximation versions of Sarsa and Q-Learning using one-hot feature encoding (Î¸Â·Ï†(s,a)).
- **`deep_rl.py`**: Deep Q-Network implementation with convolutional neural network, experience replay buffer, and target network updates.
- **`main.py`**: Runs all 7 algorithms on small frozen lake and outputs formatted results (policies and value functions).
- **`questions.py`**: Experimental code for answering questions Q1-Q3 (iteration counts on big lake, learning curves for all model-free algorithms, parameter tuning experiments).

**Generated Files:**
- **`output.txt`**: Contains the complete output from running `main.py`, including policies and value functions for all 7 algorithms. **Required for submission grading.**
- **`figures/`**: Directory containing all learning curve plots and parameter tuning visualizations generated by `questions.py`.

**Provided Files:**
- **`p.npy`**: Validation file containing correct transition probabilities for the small frozen lake. Used to verify `environment.py` implementation is correct.

**Part 2 Files:**
- **`part2/experiments.py`**: Custom reinforcement learning experiments using Minigrid environments and Stable Baselines3 algorithms (PPO, A2C, DQN).
- **`part2/analysis.py`**: Analyzes experimental results, performs statistical tests, and generates comparison plots.
- **`part2/README.md`**: Detailed documentation of Part 2 research question, methodology, and results.

---

## Installation

### Dependencies

This project requires:
- **Python 3.8+**
- **NumPy 1.24.0** - Array operations and numerical computing
- **PyTorch 2.0.0** - Deep learning framework for DQN
- **Matplotlib 3.7.0** - Plotting and visualization

**Part 2 Additional Dependencies:**
- **Gymnasium 0.29.0** - Reinforcement learning environments
- **Minigrid 2.3.0** - Grid world environments
- **Stable Baselines3 2.2.0** - Modern RL algorithms (PPO, A2C, DQN)

### Setup

Install the required dependencies:

```bash
pip install -r requirements.txt
```

Verify installation:
```bash
python -c "import numpy, torch, matplotlib; print('âœ… All dependencies installed successfully')"
```

---

## Usage

### Part 1: The Frozen Lake

#### 1. Generate Required Output File

**IMPORTANT**: Generate `output.txt` for submission using output redirection:

```bash
python main.py > output.txt
```

This will:
- Run all 7 algorithms (policy iteration, value iteration, Sarsa, Q-Learning, Linear Sarsa, Linear Q-Learning, DQN)
- Print formatted policies and value functions
- Save everything to `output.txt` (required for grading)

**Expected runtime**: ~2-5 minutes

#### 2. Run Experimental Analysis

Generate learning curves and analysis for questions Q1-Q3:

```bash
python questions.py
```

This will:
- **Q1**: Run policy iteration and value iteration on big lake, record iteration counts
- **Q2**: Generate 5 learning curve plots (one for each model-free algorithm)
- **Q3**: Run parameter tuning experiments (grid search over Î· and Îµ)
- Save all plots to `figures/` directory

**Expected runtime**: ~10-15 minutes

#### 3. Validate Environment (Optional but Recommended)

Verify your environment implementation matches `p.npy`:

```bash
python -c "
from environment import FrozenLake
import numpy as np

# Create small lake
lake = [['&', '.', '.', '.'],
        ['.', '#', '.', '#'],
        ['.', '.', '.', '#'],
        ['#', '.', '.', '\$']]

env = FrozenLake(lake, slip=0.1, max_steps=16, seed=0)

# Load correct probabilities
p_correct = np.load('p.npy')

# Check all combinations
errors = 0
for next_state in range(17):
    for state in range(17):
        for action in range(4):
            your_p = env.p(next_state, state, action)
            correct_p = p_correct[next_state, state, action]
            if not np.isclose(your_p, correct_p, atol=1e-10):
                print(f'âŒ Mismatch at p({next_state}, {state}, {action}): yours={your_p:.10f}, correct={correct_p:.10f}')
                errors += 1

if errors == 0:
    print('âœ… Environment implementation is CORRECT! All probabilities match p.npy')
else:
    print(f'âŒ Found {errors} errors. Please fix your p() method in environment.py')
"
```

**Expected output**: `âœ… Environment implementation is CORRECT!`

---

### Part 2: Extended Analysis

Navigate to Part 2 directory and run experiments:

```bash
cd part2

# Run experiments
python experiments.py

# Generate analysis
python analysis.py
```

**Research Question**: Does PPO demonstrate better sample efficiency than DQN as environment complexity increases?

**Methodology**:
- **Environments**: Minigrid-DoorKey (5x5, 8x8, 16x16)
- **Algorithms**: PPO, A2C, DQN (from Stable Baselines3)
- **Evaluation**: Sample efficiency measured by episodes to reach 90% optimal return
- **Reproducibility**: 5 random seeds per configuration

**Expected runtime**: ~30-45 minutes

For complete Part 2 documentation, see `part2/README.md`.

---

## Algorithms Implemented

### Model-Based Methods
- **Policy Iteration**: Iteratively evaluates and improves policies until convergence
- **Value Iteration**: Directly computes optimal value function using Bellman optimality equation

### Model-Free Methods (Tabular)
- **SARSA**: On-policy temporal difference learning (learns from actions taken)
- **Q-Learning**: Off-policy temporal difference learning (learns from optimal actions)

### Function Approximation
- **Linear SARSA**: SARSA with linear function approximation (Q(s,a) = Î¸Â·Ï†(s,a))
- **Linear Q-Learning**: Q-Learning with linear function approximation

### Deep Learning
- **Deep Q-Network (DQN)**: Deep reinforcement learning with convolutional neural network, experience replay, and target network

---

## Environment

The Frozen Lake environment consists of:
- **Start position** (&): Where the agent begins
- **Frozen tiles** (.): Safe to walk on
- **Holes** (#): Terminal states (fall in = episode ends)
- **Goal** ($): Terminal state with reward = 1

**Dynamics**:
- **Actions**: Up (0), Left (1), Down (2), Right (3)
- **Slip probability**: 10% chance of moving in random direction
- **Discount factor**: Î³ = 0.9
- **Episode length**: Maximum 16 steps (small lake) or 64 steps (big lake)

The agent can slip with a certain probability (0.1), making the environment stochastic and challenging.

---

## Expected Output

After running `python main.py > output.txt`, the file should contain:

```
# Model-based algorithms

## Policy iteration
Lake:
[['&' '.' '.' '.']
 ['.' '#' '.' '#']
 ['.' '.' '.' '#']
 ['#' '.' '.' '$']]
Policy:
[['_' '>' '_' '<']
 ['_' '^' '_' '^']
 ['>' '_' '_' '^']
 ['^' '>' '>' '^']]
Value:
[[0.455 0.504 0.579 0.505]
 [0.508 0.    0.653 0.   ]
 [0.584 0.672 0.768 0.   ]
 [0.    0.771 0.887 1.   ]]

## Value iteration
[... similar output ...]

# Model-free algorithms

## Sarsa
[... similar output ...]

[... continues for all 7 algorithms ...]
```

---

## Troubleshooting

**Issue**: `output.txt` is empty or missing  
**Solution**: Make sure to use output redirection: `python main.py > output.txt` (not just `python main.py`)

**Issue**: Environment validation fails  
**Solution**: Check your `p()` method implementation in `environment.py`. Pay special attention to:
- Slip probability (10% = 0.1)
- Boundary handling (hitting walls means staying in place)
- Absorbing state transitions (goal and holes â†’ absorbing state)

**Issue**: DQN doesn't converge  
**Solution**: This is expected for the small frozen lake. DQN may show suboptimal performance due to limited training data and high variance.

**Issue**: Import errors  
**Solution**: Ensure all dependencies are installed: `pip install -r requirements.txt`

---

## Authors

Group [X]  
- [Student 1 Name] - [Role]
- [Student 2 Name] - [Role]
- [Student 3 Name] - [Role]

---

## License

Academic project - Not for redistribution
